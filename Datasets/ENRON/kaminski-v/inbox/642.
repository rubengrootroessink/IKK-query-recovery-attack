Message-ID: <32192012.1075863444130.JavaMail.evans@thyme>
Date: Mon, 23 Jul 2001 09:54:01 -0700 (PDT)
From: tanya.tamarchenko@enron.com
To: j.kaminski@enron.com
Subject: FW:
Mime-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-From: Tamarchenko, Tanya </O=ENRON/OU=NA/CN=RECIPIENTS/CN=TTAMARC>
X-To: Kaminski, Vince J </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Vkamins>
X-cc: 
X-bcc: 
X-Folder: \VKAMINS (Non-Privileged)\Kaminski, Vince J\Inbox
X-Origin: Kaminski-V
X-FileName: VKAMINS (Non-Privileged).pst

Vince, I want you to be aware of the discussions between RAC, IT and Research regarding using more than 7 factors for VAR.

I presented this idea couple of months ago. This will resolve the issue of capturing term-structure of correlations across different commodities which we have for our current VAR system and which we try to overcome by some patches (joint factors for selected groups of commodities). This will not increase the amount of calculations. In fact, just the opposite, this will
decrease the number of independent random variables which drive the model from approximately 1000 to about 60-40.

RAC did not pay much attention at that time. IT was busy with other things. This task was low priority.

Recently Naveen realized that his Convolution VAR will not work unless this idea of increasing the number of factors 
is implemented. With the present system he needs to deal with 1000 independent random variables and this is not feasible
for Convolution (or Analytical) VAR. 

We had a meeting regarding this issue last week and the consensus was finally to go ahead and increase the number of factors.
This is good news because we need this change in our system for the new "Reliability VAR" as well.

What worries me that Naveen presents this change now as part of the Convolution VAR. Not only it sounds now like his
idea, but, more importantly, I am afraid that when we need the same change (60 factors) for Reliability VAR we will
have to seek IT support again and compete with RAC for resources and we always loose in this battle. In addition, the 
Monte-Carlo is not going away so the right thing to do is to implement 60 factors in our production system. 

Regards,

Tanya

 -----Original Message-----
From: 	Hayden, Frank  
Sent:	Monday, July 23, 2001 11:07 AM
To:	Andrews, Naveen; Jia, Winston; Tamarchenko, Tanya
Cc:	Basu, Nilay; Yu, Jin
Subject:	RE: 

 But don't these action items "speed up" the convolution VAR rollout?

 -----Original Message-----
From: 	Andrews, Naveen  
Sent:	Monday, July 23, 2001 11:03 AM
To:	Hayden, Frank; Jia, Winston; Tamarchenko, Tanya
Cc:	Basu, Nilay; Yu, Jin
Subject:	RE: 

It is unneccessary to do action items which Convolution VaR is going to cover in its scope, and I am of the firm opinion that it is counterproductive to do things aiming at the Current MC framework when significant changes are to be done for Convolution.

These changes include Factor Loadings and correlations:  The factor analysis will be done jointly, with more factors in the Convolution framework.
Naveen


 -----Original Message-----
From: 	Hayden, Frank  
Sent:	Monday, July 23, 2001 9:33 AM
To:	Jia, Winston; Tamarchenko, Tanya
Cc:	Andrews, Naveen; Basu, Nilay; Yu, Jin
Subject:	RE: 

This is an unexpected answer.  I thought these changes were part of project X, were they not?

Frank


 -----Original Message-----
From: 	Jia, Winston  
Sent:	Monday, July 23, 2001 9:31 AM
To:	Tamarchenko, Tanya; Hayden, Frank
Cc:	Andrews, Naveen; Basu, Nilay; Yu, Jin
Subject:	RE: 


Tanya and Frank,

The method will be first implemented for Convolution VaR.  To implement the method for current VaR (Monte Carlo method) needs many changes to existing programs, hence it will take longer.

The methodology is simple, but there are still some issues need to be confirmed/resolved, mostly related to data.

1. Data Alignment:  I understand that we will use fixed contract methodology from now on. We still need expiration rules to find prompt month for each primary curves.  Hence we still need make sure all expiration rules are set up correctly for each primary curve.

2. Data missing:  How do we handle data missing problem.  What should we do if there are some missing prices for some curves.  Do we "recreate" those missing data or do we eliminate all the prices on those missing dates?  If we "recreate", then how.  If we eliminate, then we may eliminating many good data because a few bad curves.

3. What will be the implication if we mix weekly curves with monthly curves together.  It seems OK, but we need make sure this is the case.

4. Do we still need factor look up for those inactive curves?  Or we don't care since we are using their true history.

There may be some other issues coming up when get into the design.

Regards,

Winston


 -----Original Message-----
From: 	Tamarchenko, Tanya  
Sent:	Monday, July 23, 2001 8:11 AM
To:	Hayden, Frank
Cc:	Jia, Winston
Subject:	RE: 

Frank, 
we  have discussed the implementation with Winston and Naveen. I understand that how soon we start working on it
depends on the list of IT priorities. The methodology is clear. I wish it is implemented soon.

Tanya
 -----Original Message-----
From: 	Hayden, Frank  
Sent:	Sunday, July 22, 2001 9:13 PM
To:	Tamarchenko, Tanya
Subject:	

Tanya,
When do you think we will be able to implement the new factor (term structure) methodology?

Thanks,
Frank