FROM: "Fornoville, Tom" <Tom.Fornovi...@truvo.com>
SUBJECT: RE: Relevancy Practices
DATE: 29 Apr 2010

We've only been using Lucene for a couple of weeks and we're still in
the evaluation and R&D phase but there's one single thing that has
helped us out enormously with the relevance testing: a set of reference
documents and queries. We basically sat together with the business
people a created a list of about 50 (fictional) documents, some queries
and the order in which results should be returned. Once everyone agreed
on this reference data we converted it to a set of unit tests.

Until now this approach has helped us out big time both in refining the
business requirements and the scoring and relevancy in the search engine
itself.

Cheers,
Tom

-----Original Message-----
From: Grant Ingersoll [mailto:gsiasf@gmail.com] On Behalf Of Grant
Ingersoll
Sent: donderdag 29 april 2010 16:15
To: java-user@lucene.apache.org
Subject: Relevancy Practices

I'm putting on a talk at Lucene Eurocon
(http://lucene-eurocon.org/sessions-track1-day2.html#1) on "Practical
Relevance" and I'm curious as to what people put in practice for testing
and improving relevance.  I have my own inclinations, but I don't want
to muddy the water just yet.  So, if you have a few moments, I'd love to
hear responses to the following questions.

What worked?  
What didn't work?  
What didn't you understand about it?  
What tools did you use?  
What tools did you wish you had either for debugging relevance or
"fixing" it?
How much time did you spend on it?
How did you avoid over/under tuning?
What stage of development/testing/production did you decide to do
relevance tuning?  Was that timing planned or not?


Thanks,
Grant

---------------------------------------------------------------------
To unsubscribe, e-mail: java-user-unsubscribe@lucene.apache.org
For additional commands, e-mail: java-user-help@lucene.apache.org


