FROM: "Suman Ghosh" <suman.ghos...@gmail.com>
SUBJECT: Re: StackOverflowError while calling IndexReader.deleteDocuments(new Term())
DATE: 28 Nov 2006

Mike,
I've not tried it yet, but I think the problem can be reproduced.
However, it'll take a few hours to reach that threshhold since my code
also needs to extract text from some very large PDF documents to store
in the index.

I'll post the pseudo-code of my code tomorrow. Maybe that'll help
point to mistakes I'm making in the logic.

Suman

On 11/27/06, Michael McCandless <lucene@mikemccandless.com> wrote:
> Suman Ghosh wrote:
>
> > On 11/27/06, Yonik Seeley <yonik@apache.org> wrote:
> >> On 11/27/06, Suman Ghosh <suman.ghosh.1@gmail.com> wrote:
> >> > Here are the values:
> >> >
> >> > mergeFactor=10
> >> > maxMergeDocs=100000
> >> > minMergeDocs=100
> >> >
> >> > And I see your point. At the time of the crash, I have over 5000
> >> > segments. I'll try some conservative number and try to rebuild the
> >> > index.
> >>
> >> Although I don't see how those settings can produce 5000 segments,
> >> I've developed a non-recursive patch you might want to try:
> >> https://issues.apache.org/jira/browse/LUCENE-729
>
> Suman, I'd really like to understand how you're getting so many
> segments in your index.  Is this (getting 5000 segments) easy to
> reproduce?  Are you closing / reopening your writer every so often (eg
> to delete documents or something)?
>
> Mike
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: java-user-unsubscribe@lucene.apache.org
> For additional commands, e-mail: java-user-help@lucene.apache.org
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: java-user-unsubscribe@lucene.apache.org
For additional commands, e-mail: java-user-help@lucene.apache.org


