FROM: Michael McCandless <luc...@mikemccandless.com>
SUBJECT: Re: recurrent IO/CPU peaks
DATE: 2 Mar 2011

On Tue, Mar 1, 2011 at 11:40 AM,  <v.sevel@lombardodier.com> wrote:

> we developped a real time logging system. we index 4.5 millions
> events/day, spread over multiple servers, each with its own index. every
> night with delete events from the index based on a retention policy then
> we optimize. each server takes between 1 and 2 hours to optimize. ideally,
> we would like to optimize more quickly, without compromising the search
> performances. in the lucene in action book, it says "use optimize
> sparingly; use the optimize(maxNumSegments) method instead". what is a
> reasonnable maxNumSegments in my situation?

Maybe try starting with maxNumSegments=10 and iterate from there?

But, are you sure you even need to optimize at all?  Are you hitting
search performance issues if you don't optimize?

Another thing to try is calling .setCalibrateSizeByDeletes(true) on
your LogByteSizeMergePolicy (the default).  This generally causes it
to favor merging away segments with many deletions...

-- 
Mike

http://blog.mikemccandless.com

---------------------------------------------------------------------
To unsubscribe, e-mail: java-user-unsubscribe@lucene.apache.org
For additional commands, e-mail: java-user-help@lucene.apache.org


