FROM: Doug Cutting <cutt...@lucene.com>
SUBJECT: Re: Indexing very large sets (10 million docs)
DATE: 28 Jul 2003

Ryan Clifton wrote:
> You seem to by implying that it is possible to optimize very large indexes.  My index
has a couple million records, but more importantly it's about 40 gigs in size.  I have tried
many times to optimize it and this always results in hitting the Linux file size limit.  Is
there  a way to get around this?  I have the merge factor and max merge docs set, but the
optimization process seems to ignore those fields.  

On Redhat 8.0 I have built indexes whose total size is 49GB and whose 
largest file (the .prx) is 28GB.  I haven't yet tried to build anything 
larger, so I don't know exactly where the limit is.

Doug


---------------------------------------------------------------------
To unsubscribe, e-mail: lucene-user-unsubscribe@jakarta.apache.org
For additional commands, e-mail: lucene-user-help@jakarta.apache.org


