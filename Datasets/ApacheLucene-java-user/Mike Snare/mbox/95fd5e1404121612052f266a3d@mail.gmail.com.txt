FROM: Mike Snare <mikesn...@gmail.com>
SUBJECT: Re: Why does the StandardTokenizer split hyphenated words?
DATE: 16 Dec 2004

Absolutely, but -- correct me if I'm wrong -- it would give no higher
ranking to half-baked and would take a good deal longer on large
indices.


On Thu, 16 Dec 2004 20:03:27 +0100, Daniel Naber
<daniel.naber@t-online.de> wrote:
> On Thursday 16 December 2004 13:46, Mike Snare wrote:
> 
> > > Maybe for "a-b", but what about English words like "half-baked"?
> >
> > Perhaps that's the difference in thinking, then.  I would imagine that
> > you would want to search on "half-baked" and not "half AND baked".
> 
> A search for half-baked will find both half-baked and "half baked" (the
> phrase). The only thing you'll not find if halfbaked.
> 
> Regards
>  Daniel
> 
> --
> http://www.danielnaber.de
> 
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: lucene-user-unsubscribe@jakarta.apache.org
> For additional commands, e-mail: lucene-user-help@jakarta.apache.org
> 
>

---------------------------------------------------------------------
To unsubscribe, e-mail: lucene-user-unsubscribe@jakarta.apache.org
For additional commands, e-mail: lucene-user-help@jakarta.apache.org


