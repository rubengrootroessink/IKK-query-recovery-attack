FROM: "Karthik N S" <kart...@controlnet.co.in>
SUBJECT: RE: Downloading Full Copies of Web Pages
DATE: 20 Oct 2004

Hi


Try

    nutch   [ http://www.nutch.org/docs/en/about.html ]  underneath it uses
Lucene....  :)





-----Original Message-----
From: Luciano Barbosa [mailto:lab@sci.utah.edu]
Sent: Wednesday, October 20, 2004 3:06 AM
To: lucene-user@jakarta.apache.org
Subject: Downloading Full Copies of Web Pages


Hi folks,
I want to download full copies of web pages and storage them locally as
well the hyperlink structures as local directories. I tried to use
Lucene, but I've realized that  it doesn't have a crawler.
Does anyone know a software that make this?
Thanks,

---------------------------------------------------------------------
To unsubscribe, e-mail: lucene-user-unsubscribe@jakarta.apache.org
For additional commands, e-mail: lucene-user-help@jakarta.apache.org


---------------------------------------------------------------------
To unsubscribe, e-mail: lucene-user-unsubscribe@jakarta.apache.org
For additional commands, e-mail: lucene-user-help@jakarta.apache.org


