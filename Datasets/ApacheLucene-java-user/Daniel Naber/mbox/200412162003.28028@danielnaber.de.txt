FROM: Daniel Naber <daniel.na...@t-online.de>
SUBJECT: Re: Why does the StandardTokenizer split hyphenated words?
DATE: 16 Dec 2004

On Thursday 16 December 2004 13:46, Mike Snare wrote:

> > Maybe for "a-b", but what about English words like "half-baked"?
>
> Perhaps that's the difference in thinking, then. Â I would imagine that
> you would want to search on "half-baked" and not "half AND baked".

A search for half-baked will find both half-baked and "half baked" (the 
phrase). The only thing you'll not find if halfbaked.

Regards
 Daniel

-- 
http://www.danielnaber.de

---------------------------------------------------------------------
To unsubscribe, e-mail: lucene-user-unsubscribe@jakarta.apache.org
For additional commands, e-mail: lucene-user-help@jakarta.apache.org


